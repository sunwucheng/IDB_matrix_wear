{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDB_matrix_wear.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sMZkCFSHInU"
      },
      "source": [
        "# Wear evaluation of WC-Cu matrix composite based on SEM images\r\n",
        "# WC-Cu基胎体磨损电镜图像识别"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdfnId65HtDD"
      },
      "source": [
        "> 来源/Source：[Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) in [Detectron2 project](https://github.com/facebookresearch/detectron2)\r\n",
        "\r\n",
        "> 参考/Reference1：[dyh/unbox_detecting_tunnel_fissure](https://github.com/dyh/unbox_detecting_tunnel_fissure)\r\n",
        "\r\n",
        "> 参考/Reference2：[TannerGilbert/Detectron2-Train-a-Instance-Segmentation-Model](https://github.com/TannerGilbert/Detectron2-Train-a-Instance-Segmentation-Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEAcAMThvuNT"
      },
      "source": [
        "## 0. Google Drive (storing data) + Google Colab (computing power)\r\n",
        "## 0. Google Drive (数据储存) + Google Colab (算力平台)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6v5NMrx0Zl"
      },
      "source": [
        "### 0.1 Check out GPU info \r\n",
        "### 0.1 检查GPU信息\r\n",
        "> if not GPU, click \"Runtime\"→\"change runtime type\"→\"Hardware accelerator\"→\"GPU\"→\"SAVE\"\r\n",
        "\r\n",
        "> 不是GPU的话请点击“代码执行程序”→“更改运行时类型”→“硬件加速器”→“GPU”→保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozWNVCwHZks"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR3JWY5Wx6MM"
      },
      "source": [
        "### 0.2 mount google drive folder\r\n",
        "### 0.2 装载GoogleDrive云端网盘\r\n",
        "> 在打开链接弹出的窗口中复制验证码后出现的输入框后回车"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1yOLQBwzyl"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OeDDyMx9TM"
      },
      "source": [
        "### 0.3 Import labelled images\r\n",
        "### 0.3 导入标注过的图像数据集文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7GcEbiaw9xV"
      },
      "source": [
        "!git clone 'https://github.com/sunwucheng/IDB_matrix_wear.git' '/content/drive/MyDrive/IDB_matrix_wear'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Y4ICguIHxd"
      },
      "source": [
        "## 1. Install Mask R-CNN framework\r\n",
        "## 1. 安装Mask R-CNN算法框架"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwQNUwvuM2Pj"
      },
      "source": [
        "### 1.1 Install dependencies\r\n",
        "### 1.1 安装依赖项"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFahu2haKRcC"
      },
      "source": [
        "# install dependencies: \r\n",
        "!pip install pyyaml==5.1\r\n",
        "import torch, torchvision\r\n",
        "print(torch.__version__, torch.cuda.is_available())\r\n",
        "!gcc --version\r\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y71On6OGM5c1"
      },
      "source": [
        "### 1.2 Install detectron2\r\n",
        "### 1.2 安装detectron2\r\n",
        "> click **[ RESTART RUNTIME ]** button at the end after all text printed\r\n",
        "\r\n",
        "> 输出完文本后点击其末尾的**[ RESTART RUNTIME ]**按钮重启"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxHL_jcOKJA7"
      },
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\r\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\r\n",
        "import torch\r\n",
        "assert torch.__version__.startswith(\"1.7\")\r\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\r\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBzyiYvVYmS"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "***please make sure of that you have click the [ RESTART RUNTIME ] button -> [ YES ] button to restart colab runtime***\r\n",
        "\r\n",
        "***请确认您点击了 [ RESTART RUNTIME ] 按钮 -> [ 是 ] 按钮，来重新加载 colab 运行时***\r\n",
        "\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95TfTm-9M9-T"
      },
      "source": [
        "### 1.3 Import modules\r\n",
        "### 1.3 导入模块"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhRIURvXJ3Xk"
      },
      "source": [
        "# Some basic setup:\r\n",
        "# Setup detectron2 logger\r\n",
        "import detectron2\r\n",
        "from detectron2.utils.logger import setup_logger\r\n",
        "setup_logger()\r\n",
        "\r\n",
        "# import some common libraries\r\n",
        "import numpy as np\r\n",
        "import os, json, cv2, random\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# import some common detectron2 utilities\r\n",
        "from detectron2 import model_zoo\r\n",
        "from detectron2.engine import DefaultTrainer\r\n",
        "from detectron2.engine import DefaultPredictor\r\n",
        "from detectron2.config import get_cfg\r\n",
        "from detectron2.utils.visualizer import ColorMode\r\n",
        "from detectron2.utils.visualizer import Visualizer\r\n",
        "from detectron2.utils.visualizer import GenericMask\r\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\r\n",
        "from detectron2.data import build_detection_test_loader\r\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\r\n",
        "from detectron2.structures import BoxMode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxxz2g3vIXU_"
      },
      "source": [
        "## 2. Register the datasets\r\n",
        "## 2. 注册数据集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a83DdQfX2Ohl"
      },
      "source": [
        "### 2.1 Use this if you label with labelme\r\n",
        "### 2.1 注册图像数据集(Labelme标注的用这个)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_RwaV02WzB"
      },
      "source": [
        "def get_wear_dicts(directory):\r\n",
        "  classes = ['abrasive wear', 'wear debris', 'fatigue wear']\r\n",
        "  dataset_dicts = []\r\n",
        "  for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\r\n",
        "      json_file = os.path.join(directory, filename)\r\n",
        "      with open(json_file) as f:\r\n",
        "          img_anns = json.load(f)\r\n",
        "\r\n",
        "      record = {}\r\n",
        "      \r\n",
        "      filename = os.path.join(directory, img_anns[\"imagePath\"])\r\n",
        "      \r\n",
        "      record[\"file_name\"] = filename\r\n",
        "      record[\"height\"] = 1088\r\n",
        "      record[\"width\"] = 1024\r\n",
        "    \r\n",
        "      annos = img_anns[\"shapes\"]\r\n",
        "      objs = []\r\n",
        "      for anno in annos:\r\n",
        "          px = [a[0] for a in anno['points']]\r\n",
        "          py = [a[1] for a in anno['points']]\r\n",
        "          poly = [(x, y) for x, y in zip(px, py)]\r\n",
        "          poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "          obj = {\r\n",
        "              \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "              \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "              \"segmentation\": [poly],\r\n",
        "              \"category_id\": classes.index(anno['label']),\r\n",
        "              \"iscrowd\": 0\r\n",
        "          }\r\n",
        "          objs.append(obj)\r\n",
        "      record[\"annotations\"] = objs\r\n",
        "      dataset_dicts.append(record)\r\n",
        "  return dataset_dicts\r\n",
        "\r\n",
        "for d in [\"train\", \"val\"]:\r\n",
        "  DatasetCatalog.register(\"wear_\" + d, lambda d=d: get_wear_dicts(\"/content/drive/MyDrive/IDB_matrix_wear/\" + d))\r\n",
        "  MetadataCatalog.get(\"wear_\" + d).set(thing_classes=['abrasive wear', 'wear debris', 'fatigue wear'])\r\n",
        "wear_metadata = MetadataCatalog.get(\"wear_train\")\r\n",
        "\r\n",
        "print('Complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ4xBHryNNPi"
      },
      "source": [
        "### 2.2 Visualize the annotations of randomly selected samples in the training set\r\n",
        "### 2.2 随机预览训练集图像分割方式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wGY0W4TMur7"
      },
      "source": [
        "dataset_dicts = get_wear_dicts(\"/content/drive/MyDrive/IDB_matrix_wear/train\")\r\n",
        "for d in random.sample(dataset_dicts, 2):\r\n",
        "    img = cv2.imread(d[\"file_name\"])\r\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wear_metadata, scale=0.5)\r\n",
        "    out = visualizer.draw_dataset_dict(d)\r\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_PqdEIbyiWO"
      },
      "source": [
        "### 2.3 Save ground truth in training and validation dataset in folder\r\n",
        "### 2.3 保存训练集、验证集图片的标注结果输出到文件夹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artsWnHum_2x"
      },
      "source": [
        "def get_dir_true_info(directory):\r\n",
        "  os.mkdir(directory+'_true')\r\n",
        "  csv_file = open(directory+'_true/info.csv', 'w')\r\n",
        "  csv_file.close()\r\n",
        "  text_title = 'file_name,height,width,pixel,instance_num,abrasive_area,debris_area,fatigue_area,adhesive_area,overlap_area'\r\n",
        "  with open(directory+'_true/info.csv','a+',encoding='utf-8') as f:\r\n",
        "    f.write(text_title+'\\n')\r\n",
        "    f.close()\r\n",
        "  dataset_dicts = get_wear_dicts(directory)\r\n",
        "  for d in dataset_dicts:\r\n",
        "    file = d[\"file_name\"]\r\n",
        "    file_name = file.split(\"/\")[-1]\r\n",
        "    true_name = directory+'_true/'+file_name\r\n",
        "    img = cv2.imread(d[\"file_name\"])\r\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wear_metadata, scale=1.0)\r\n",
        "    out = visualizer.draw_dataset_dict(d)\r\n",
        "    # cv2_imshow(out.get_image()[:, :, ::-1])\r\n",
        "    cv2.imwrite(true_name, out.get_image()[:, :, ::-1])\r\n",
        "  dir_info = []\r\n",
        "  for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\r\n",
        "    json_file = os.path.join(directory, filename)\r\n",
        "    with open(json_file) as f:\r\n",
        "      img_json = json.load(f)\r\n",
        "      f.close()\r\n",
        "    annos = img_json[\"shapes\"]\r\n",
        "    annos_num = len(annos)\r\n",
        "    img_name = img_json[\"imagePath\"]\r\n",
        "    img_height = img_json[\"imageHeight\"]\r\n",
        "    img_width = img_json[\"imageWidth\"]\r\n",
        "    img_pixel = img_height * img_width\r\n",
        "    abrasive_mask = np.zeros((img_height, img_width))\r\n",
        "    debris_mask = np.zeros((img_height, img_width))\r\n",
        "    fatigue_mask = np.zeros((img_height, img_width))\r\n",
        "    full_mask1 = np.ones((img_height, img_width))\r\n",
        "    full_mask1_emp = np.zeros(((img_height - img_width),img_width))\r\n",
        "    full_mask1[img_width:img_height,:]=full_mask1_emp\r\n",
        "    adhesive_mask = full_mask1\r\n",
        "    for i, x in enumerate(annos):\r\n",
        "      polygon_mask = np.zeros((img_height, img_width))\r\n",
        "      points = x[\"points\"]\r\n",
        "      pts = np.array(points, dtype=np.int32)\r\n",
        "      cv2.fillConvexPoly(polygon_mask, pts, 1)\r\n",
        "      if x['label'] == 'abrasive wear':\r\n",
        "        abrasive_mask = np.array(abrasive_mask) + np.array(polygon_mask)\r\n",
        "        abrasive_mask[abrasive_mask > 0] = 1\r\n",
        "      elif x['label'] == 'wear debris':\r\n",
        "        debris_mask = np.array(debris_mask) + np.array(polygon_mask)\r\n",
        "        debris_mask[debris_mask > 0] = 1\r\n",
        "      elif x['label'] == 'fatigue wear':\r\n",
        "        fatigue_mask = np.array(fatigue_mask) + np.array(polygon_mask)\r\n",
        "        fatigue_mask[fatigue_mask > 0] = 1\r\n",
        "    abrasiveANDfatigue_mask = np.array(abrasive_mask) + np.array(fatigue_mask)\r\n",
        "    abrasiveANDfatigue_mask[abrasiveANDfatigue_mask > 0] = 1\r\n",
        "    abrasiveDIFfatigue_mask = np.array(abrasive_mask) + np.array(fatigue_mask)\r\n",
        "    abrasiveDIFfatigue_mask[abrasiveDIFfatigue_mask < 2] = 0\r\n",
        "    abrasiveDIFfatigue_mask[abrasiveDIFfatigue_mask == 2] = 1\r\n",
        "    adhesive_mask = np.array(adhesive_mask) - np.array(abrasiveANDfatigue_mask)\r\n",
        "    overlap_mask = abrasiveDIFfatigue_mask\r\n",
        "    abrasive_area = np.sum(abrasive_mask != 0)\r\n",
        "    debris_area = np.sum(debris_mask != 0)\r\n",
        "    fatigue_area = np.sum(fatigue_mask != 0)\r\n",
        "    adhesive_area = np.sum(adhesive_mask != 0)\r\n",
        "    overlap_area = np.sum(overlap_mask != 0)\r\n",
        "    image_info = {\"img_name\":img_name, \"abrasive_mask\":abrasive_mask,\"debris_mask\":debris_mask,\"fatigue_mask\":fatigue_mask,\"adhesive_mask\":adhesive_mask,\"overlap_mask\":overlap_mask}\r\n",
        "    dir_info.append(image_info)\r\n",
        "    true_text = str(img_name)+','+str(img_height)+','+str(img_width)+','+str(img_pixel)+','+str(annos_num)+','+str(abrasive_area)+','+str(debris_area)+','+str(fatigue_area)+','+str(adhesive_area)+','+str(overlap_area)\r\n",
        "    with open((directory+'_true/info.csv'),'a+',encoding='utf-8') as f:\r\n",
        "      f.write(true_text+'\\n')\r\n",
        "      f.close()\r\n",
        "  return dir_info\r\n",
        "\r\n",
        "train_true_info = get_dir_true_info('/content/drive/MyDrive/IDB_matrix_wear/train')\r\n",
        "val_true_info = get_dir_true_info('/content/drive/MyDrive/IDB_matrix_wear/val')\r\n",
        "print('Complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWz9S9EMk97"
      },
      "source": [
        "## 3. Mask R-CNN → Instance Segmentation\r\n",
        "## 3. Mask R-CNN实例分割训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxqkshEw_AnM"
      },
      "source": [
        "### 3.1 Train or use the pre-trained Mask R-CNN model (Pick one between 3.1A and 3.1B)\r\n",
        "### 3.1 训练或者使用预训练好的模型 (在3.1A和3.1B中选择一种)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ44lZMn2Wqq"
      },
      "source": [
        "#### 3.1A Use the pre-trained model by default\r\n",
        "#### 3.1A 默认使用预选连模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCJGuIllhsoR"
      },
      "source": [
        "# Use the pre-trained model directly （Take less time for downloading the model)\r\n",
        "model_path = '/content/drive/MyDrive/IDB_matrix_wear/output/'\r\n",
        "if not os.path.exists(model_path):\r\n",
        "  os.mkdir(model_path)\r\n",
        "!wget -P '/content/drive/MyDrive/IDB_matrix_wear' https://github.com/sunwucheng/IDB_matrix_wear/releases/download/v1.1/output.zip \r\n",
        "!unzip '/content/drive/MyDrive/IDB_matrix_wear/output.zip' -d '/content/drive/MyDrive/IDB_matrix_wear'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Uas1zQ3IRo"
      },
      "source": [
        "#### 3.1B Train the model by yourself\r\n",
        "#### 3.1B 自己训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGx-nnrk-MUB"
      },
      "source": [
        "# Train the model yourself （Take much more time for training the model)\r\n",
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "cfg.DATASETS.TRAIN = (\"wear_train\",)\r\n",
        "cfg.DATASETS.TEST = ()\r\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\r\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") \r\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\r\n",
        "cfg.SOLVER.BASE_LR = 0.00025   # pick a good LR\r\n",
        "cfg.SOLVER.MAX_ITER = 10000   # you will need to train longer for a practical dataset\r\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # default: 512\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3   \r\n",
        "\r\n",
        "cfg.OUTPUT_DIR = '/content/drive/MyDrive/IDB_matrix_wear/output/'\r\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\r\n",
        "trainer = DefaultTrainer(cfg) \r\n",
        "trainer.resume_or_load(resume=False)\r\n",
        "trainer.train()\r\n",
        "print('train done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS_L19mSScuB"
      },
      "source": [
        "### 3.2 Check the tensorboard of the trained model\r\n",
        "### 3.2 查看所训练模型的参数曲线"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HO0Rcy7STRS"
      },
      "source": [
        "# Look at training curves in tensorboard:\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir '/content/drive/MyDrive/IDB_matrix_wear/output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1WQjROTN-e7"
      },
      "source": [
        "### 3.3 Inference!\r\n",
        "### 3.3 检测!\r\n",
        "> \r\n",
        "\r\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ML7aT86_oZ-"
      },
      "source": [
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  \r\n",
        "cfg.MODEL.WEIGHTS = os.path.join('/content/drive/MyDrive/IDB_matrix_wear/output', \"model_final.pth\")\r\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\r\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwFmpT3d57pH"
      },
      "source": [
        "def get_dir_pred_info(directory):\r\n",
        "  os.mkdir(directory+'_pred')\r\n",
        "  csv_file = open(directory+'_pred/info.csv', 'w')\r\n",
        "  text_title = 'file_name,height,width,pixel,instance_num,abrasive_area,debris_area,fatigue_area,adhesive_area,overlap_area'\r\n",
        "  with open(directory+'_pred/info.csv','a+',encoding='utf-8') as f:\r\n",
        "    f.write(text_title+'\\n')\r\n",
        "    f.close()\r\n",
        "  dir_info = []\r\n",
        "  files = os.listdir(directory)\r\n",
        "  files.sort()\r\n",
        "  for file_name in files:\r\n",
        "    # filter jpg and tiff files\r\n",
        "    if file_name[-4:] == '.jpg' or file_name[-5:] == '.tiff':\r\n",
        "      image_path = os.path.join(directory, file_name)\r\n",
        "      im = cv2.imread(image_path)\r\n",
        "      outputs = predictor(im)          \r\n",
        "      predictions = outputs[\"instances\"].to(\"cpu\")\r\n",
        "      predictions_classes = np.asarray(predictions.pred_classes)\r\n",
        "      predictions_scores = np.asarray(predictions.scores)\r\n",
        "      predictions_masks = np.asarray(predictions.pred_masks)\r\n",
        "      predictions_areas = []\r\n",
        "      predictions_instance_num = predictions_masks.shape[0]\r\n",
        "      predictions_height = predictions_masks.shape[1]\r\n",
        "      predictions_width = predictions_masks.shape[2]\r\n",
        "      predictions_pixel = predictions_height * predictions_width          \r\n",
        "      blank_mask = np.zeros((predictions_height, predictions_width))\r\n",
        "      full_mask = np.ones((predictions_height, predictions_width))\r\n",
        "      full_mask_emp = np.zeros(((predictions_height - predictions_width),predictions_width))\r\n",
        "      full_mask[predictions_width:predictions_height,:]=full_mask_emp\r\n",
        "      predictions_abrasive_mask = blank_mask\r\n",
        "      predictions_debris_mask = blank_mask\r\n",
        "      predictions_fatigue_mask = blank_mask\r\n",
        "      prediction_adhesive_mask = full_mask\r\n",
        "      for i in range(0, predictions_instance_num):\r\n",
        "        instance_num = i\r\n",
        "        instance_class = predictions_classes[i]\r\n",
        "        instance_score = predictions_scores[i]\r\n",
        "        instance_mask = predictions_masks[i]\r\n",
        "        instance_area = np.sum(instance_mask != 0)\r\n",
        "        predictions_areas.append(instance_area)\r\n",
        "        if predictions_classes[i] == 0:\r\n",
        "          predictions_abrasive_mask = np.array(predictions_abrasive_mask) + np.array(instance_mask)\r\n",
        "          predictions_abrasive_mask[predictions_abrasive_mask > 0] = 1\r\n",
        "        elif predictions_classes[i] == 1:\r\n",
        "          predictions_debris_mask = np.array(predictions_debris_mask) + np.array(instance_mask)\r\n",
        "          predictions_debris_mask[predictions_debris_mask > 0] = 1\r\n",
        "        elif predictions_classes[i] == 2:\r\n",
        "          predictions_fatigue_mask = np.array(predictions_fatigue_mask) + np.array(instance_mask)\r\n",
        "          predictions_fatigue_mask[predictions_fatigue_mask > 0] = 1\r\n",
        "      abrasiveANDfatigue_mask2 = np.array(predictions_abrasive_mask) + np.array(predictions_fatigue_mask)\r\n",
        "      abrasiveANDfatigue_mask2[abrasiveANDfatigue_mask2 > 0] = 1\r\n",
        "      abrasiveDIFfatigue_mask2 = np.array(predictions_abrasive_mask) + np.array(predictions_fatigue_mask)\r\n",
        "      abrasiveDIFfatigue_mask2[abrasiveDIFfatigue_mask2 < 2] = 0\r\n",
        "      abrasiveDIFfatigue_mask2[abrasiveDIFfatigue_mask2 == 2] = 1\r\n",
        "      predictions_adhesive_mask = np.array(prediction_adhesive_mask) - np.array(abrasiveANDfatigue_mask2)\r\n",
        "      predictions_overlap_mask = abrasiveDIFfatigue_mask2\r\n",
        "      predictions_abrasive_area = np.sum(predictions_abrasive_mask != 0)\r\n",
        "      predictions_debris_area = np.sum(predictions_debris_mask != 0)\r\n",
        "      predictions_fatigue_area = np.sum(predictions_fatigue_mask != 0)\r\n",
        "      predictions_adhesive_area = np.sum(predictions_adhesive_mask != 0)\r\n",
        "      predictions_overlap_area = np.sum(predictions_overlap_mask != 0)\r\n",
        "      image_info = {\"img_name\":file_name, \"abrasive_mask\":predictions_abrasive_mask,\"debris_mask\":predictions_debris_mask,\"fatigue_mask\":predictions_fatigue_mask,\"adhesive_mask\":predictions_adhesive_mask,\"overlap_mask\":predictions_overlap_mask}\r\n",
        "      dir_info.append(image_info)\r\n",
        "      predictions_text = str(file_name)+','+str(predictions_height)+','+str(predictions_width)+','+str(predictions_pixel)+','+str(predictions_instance_num)+','+str(predictions_abrasive_area)+','+str(predictions_debris_area)+','+str(predictions_fatigue_area)+','+str(predictions_adhesive_area)+','+str(predictions_overlap_area)\r\n",
        "      with open(directory+'_pred/info.csv','a+',encoding='utf-8') as f:\r\n",
        "        f.write(predictions_text+'\\n')\r\n",
        "        f.close()\r\n",
        "      v = Visualizer(im[:,:,::-1], metadata=wear_metadata, scale=1.0, instance_mode=ColorMode.IMAGE_BW)\r\n",
        "      out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\r\n",
        "      image_obj = out.get_image()[:, :, ::-1]\r\n",
        "      # cv2_imshow(image_obj)\r\n",
        "      cv2.imwrite(os.path.join((directory+'_pred'), file_name), image_obj)\r\n",
        "  return dir_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5F_KwCw4ks-"
      },
      "source": [
        "train_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_matrix_wear/train')\n",
        "val_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_matrix_wear/val')\n",
        "test_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_matrix_wear/test')\n",
        "print('Complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDQueLp68v6P"
      },
      "source": [
        "### 3.4 Evaluation of the prediction results of images in traininglidation set\r\n",
        "### 3.4 评价训练集和验证集中图片的检测效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYLet_krQBH"
      },
      "source": [
        "!touch '/content/drive/MyDrive/IDB_matrix_wear/train_count.csv'\n",
        "!touch '/content/drive/MyDrive/IDB_matrix_wear/val_count.csv'\n",
        "text_title = 'img_name,abrasive_mask_iou,debris_mask_iou,fatigue_mask_iou,adhesive_mask_iou,abrasive_area_loss,debris_area_loss,fatigue_area_loss,adhesive_area_loss'\n",
        "with open('/content/drive/MyDrive/IDB_matrix_wear/train_count.csv','a+',encoding='utf-8') as f:\n",
        "  f.write(text_title+'\\n')\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/IDB_matrix_wear/val_count.csv','a+',encoding='utf-8') as f:\n",
        "  f.write(text_title+'\\n')\n",
        "  f.close()\n",
        "\n",
        "def mask_iou(mask1, mask2):\n",
        "  area1 = mask1.sum()\n",
        "  area2 = mask2.sum()\n",
        "  if area1 + area2 == 0:\n",
        "    mask_iou = 1\n",
        "  else:\n",
        "    inter = ((mask1+mask2)==2).sum()\n",
        "    mask_iou = inter / (area1+area2-inter)\n",
        "  return mask_iou\n",
        "\n",
        "def area_loss(mask1, mask2):\n",
        "  area1 = mask1.sum()\n",
        "  area2 = mask2.sum()\n",
        "  if area1 + area2 == 0:\n",
        "    area_loss = 0\n",
        "  else:\n",
        "    area_loss = abs(area1 - area2)/(area1 + area2)\n",
        "  return area_loss\n",
        "\n",
        "for i, x in enumerate(train_true_info):\n",
        "  img_name = x[\"img_name\"]\n",
        "  true_abrasive_mask = x[\"abrasive_mask\"]\n",
        "  true_debris_mask = x[\"debris_mask\"]\n",
        "  true_fatigue_mask = x[\"fatigue_mask\"]\n",
        "  true_adhesive_mask = x[\"adhesive_mask\"]\n",
        "  pred_abrasive_mask = train_pred_info[i][\"abrasive_mask\"]\n",
        "  pred_debris_mask = train_pred_info[i][\"debris_mask\"]\n",
        "  pred_fatigue_mask = train_pred_info[i][\"fatigue_mask\"]\n",
        "  pred_adhesive_mask = train_pred_info[i][\"adhesive_mask\"]\n",
        "  train_abrasive_iou = mask_iou(true_abrasive_mask, pred_abrasive_mask)\n",
        "  train_debris_iou = mask_iou(true_debris_mask, pred_debris_mask)\n",
        "  train_fatigue_iou = mask_iou(true_fatigue_mask, pred_fatigue_mask)\n",
        "  train_adhesive_iou = mask_iou(true_adhesive_mask, pred_adhesive_mask)\n",
        "  train_abrasive_loss = area_loss(true_abrasive_mask, pred_abrasive_mask)\n",
        "  train_debris_loss = area_loss(true_debris_mask, pred_debris_mask)\n",
        "  train_fatigue_loss = area_loss(true_fatigue_mask, pred_fatigue_mask)\n",
        "  train_adhesive_loss = area_loss(true_adhesive_mask, pred_adhesive_mask)\n",
        "  count_text = str(img_name)+','+str(train_abrasive_iou)+','+str(train_debris_iou)+','+str(train_fatigue_iou)+','+str(train_adhesive_iou)+','+str(train_abrasive_loss)+','+str(train_debris_loss)+','+str(train_fatigue_loss)+','+str(train_adhesive_loss)\n",
        "  with open('/content/drive/MyDrive/IDB_matrix_wear/train_count.csv','a+',encoding='utf-8') as f:\n",
        "    f.write(count_text+'\\n')\n",
        "    f.close()\n",
        "\n",
        "for i, x in enumerate(val_true_info):\n",
        "  img_name = x[\"img_name\"]\n",
        "  true_abrasive_mask = x[\"abrasive_mask\"]\n",
        "  true_debris_mask = x[\"debris_mask\"]\n",
        "  true_fatigue_mask = x[\"fatigue_mask\"]\n",
        "  true_adhesive_mask = x[\"adhesive_mask\"]\n",
        "  pred_abrasive_mask = val_pred_info[i][\"abrasive_mask\"]\n",
        "  pred_debris_mask = val_pred_info[i][\"debris_mask\"]\n",
        "  pred_fatigue_mask = val_pred_info[i][\"fatigue_mask\"]\n",
        "  pred_adhesive_mask = val_pred_info[i][\"adhesive_mask\"]\n",
        "  val_abrasive_iou = mask_iou(true_abrasive_mask, pred_abrasive_mask)\n",
        "  val_debris_iou = mask_iou(true_debris_mask, pred_debris_mask)\n",
        "  val_fatigue_iou = mask_iou(true_fatigue_mask, pred_fatigue_mask)\n",
        "  val_adhesive_iou = mask_iou(true_adhesive_mask, pred_adhesive_mask)\n",
        "  val_abrasive_loss = area_loss(true_abrasive_mask, pred_abrasive_mask)\n",
        "  val_debris_loss = area_loss(true_debris_mask, pred_debris_mask)\n",
        "  val_fatigue_loss = area_loss(true_fatigue_mask, pred_fatigue_mask)\n",
        "  val_adhesive_loss = area_loss(true_adhesive_mask, pred_adhesive_mask)\n",
        "  count_text = str(img_name)+','+str(val_abrasive_iou)+','+str(val_debris_iou)+','+str(val_fatigue_iou)+','+str(val_adhesive_iou)+','+str(val_abrasive_loss)+','+str(val_debris_loss)+','+str(val_fatigue_loss)+','+str(val_adhesive_loss)\n",
        "  with open('/content/drive/MyDrive/IDB_matrix_wear/val_count.csv','a+',encoding='utf-8') as f:\n",
        "    f.write(count_text+'\\n')\n",
        "    f.close()\n",
        "print('Complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljAN0hp79gTp"
      },
      "source": [
        "### 3.5 Check out the results and effects in Google Drive \r\n",
        "### 3.5 在谷歌云盘中查看预测结果和效果"
      ]
    }
  ]
}
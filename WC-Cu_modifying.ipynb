{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WC_Cu_modifying.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "31HrFEQqNH4x",
        "AvetkXuMNgg7",
        "XK4jJaOXOKCz"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sMZkCFSHInU"
      },
      "source": [
        "# Wear evaluation of WC-Cu matrix composite based on SEM images\r\n",
        "# WC-Cu基胎体磨损电镜图像识别"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdfnId65HtDD"
      },
      "source": [
        "> 来源/Source：[Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) in [Detectron2 project](https://github.com/facebookresearch/detectron2)\r\n",
        "\r\n",
        "> 参考/Reference1：[dyh/unbox_detecting_tunnel_fissure](https://github.com/dyh/unbox_detecting_tunnel_fissure)\r\n",
        "\r\n",
        "> 参考/Reference2：[TannerGilbert/Detectron2-Train-a-Instance-Segmentation-Model](https://github.com/TannerGilbert/Detectron2-Train-a-Instance-Segmentation-Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEAcAMThvuNT"
      },
      "source": [
        "## 0. Google Drive (storing data) + Google Colab (computing power)\r\n",
        "## 0. Google Drive (数据储存) + Google Colab (算力平台)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6v5NMrx0Zl"
      },
      "source": [
        "### 0.1 Check out GPU info \r\n",
        "### 0.1 检查GPU信息\r\n",
        "> if not GPU, click \"Runtime\"→\"change runtime type\"→\"Hardware accelerator\"→\"GPU\"→\"SAVE\"\r\n",
        "\r\n",
        "> 不是GPU的话请点击“代码执行程序”→“更改运行时类型”→“硬件加速器”→“GPU”→保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozWNVCwHZks"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR3JWY5Wx6MM"
      },
      "source": [
        "### 0.2 mount google drive folder\r\n",
        "### 0.2 装载GoogleDrive云端网盘\r\n",
        "> 在打开链接弹出的窗口中复制验证码后出现的输入框后回车"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1yOLQBwzyl"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OeDDyMx9TM"
      },
      "source": [
        "### 0.3 Import labelled images\r\n",
        "### 0.3 导入标注过的图像数据集文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7GcEbiaw9xV"
      },
      "source": [
        "!git clone 'https://github.com/sunwucheng/IDB_Wear.git' '/content/drive/MyDrive/IDB_Wear'\r\n",
        "!wget -P '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108' https://github.com/sunwucheng/IDB_Wear/releases/download/v0.1/WC-Cu_20210108_output.zip\r\n",
        "!unzip '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/WC-Cu_20210108_output.zip' -d '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Y4ICguIHxd"
      },
      "source": [
        "## 1. Install Mask R-CNN framework\r\n",
        "## 1. 安装Mask R-CNN算法框架"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwQNUwvuM2Pj"
      },
      "source": [
        "### 1.1 Install dependencies\r\n",
        "### 1.1 安装依赖项"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFahu2haKRcC"
      },
      "source": [
        "# install dependencies: \r\n",
        "!pip install pyyaml==5.1\r\n",
        "import torch, torchvision\r\n",
        "print(torch.__version__, torch.cuda.is_available())\r\n",
        "!gcc --version\r\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y71On6OGM5c1"
      },
      "source": [
        "### 1.2 Install detectron2\r\n",
        "### 1.2 安装detectron2\r\n",
        "> click **[ RESTART RUNTIME ]** button at the end after all text printed\r\n",
        "\r\n",
        "> 输出完文本后点击其末尾的**[ RESTART RUNTIME ]**按钮重启"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxHL_jcOKJA7"
      },
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\r\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\r\n",
        "import torch\r\n",
        "assert torch.__version__.startswith(\"1.7\")\r\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\r\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBzyiYvVYmS"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "***please make sure of that you have click the [ RESTART RUNTIME ] button -> [ YES ] button to restart colab runtime***\r\n",
        "\r\n",
        "***请确认您点击了 [ RESTART RUNTIME ] 按钮 -> [ 是 ] 按钮，来重新加载 colab 运行时***\r\n",
        "\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95TfTm-9M9-T"
      },
      "source": [
        "### 1.3 Import modules\r\n",
        "### 1.3 导入模块"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhRIURvXJ3Xk"
      },
      "source": [
        "# Some basic setup:\r\n",
        "# Setup detectron2 logger\r\n",
        "import detectron2\r\n",
        "from detectron2.utils.logger import setup_logger\r\n",
        "setup_logger()\r\n",
        "\r\n",
        "# import some common libraries\r\n",
        "import numpy as np\r\n",
        "import os, json, cv2, random\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# import some common detectron2 utilities\r\n",
        "from detectron2 import model_zoo\r\n",
        "from detectron2.engine import DefaultTrainer\r\n",
        "from detectron2.engine import DefaultPredictor\r\n",
        "from detectron2.config import get_cfg\r\n",
        "from detectron2.utils.visualizer import ColorMode\r\n",
        "from detectron2.utils.visualizer import Visualizer\r\n",
        "from detectron2.utils.visualizer import GenericMask\r\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\r\n",
        "from detectron2.data import build_detection_test_loader\r\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\r\n",
        "from detectron2.structures import BoxMode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxxz2g3vIXU_"
      },
      "source": [
        "## 2. Register the datasets\r\n",
        "## 2. 注册数据集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31HrFEQqNH4x"
      },
      "source": [
        "### 2.1A Register with this one if you label with VGG IMage Annotator (VIA)\r\n",
        "### 2.1A 注册图像数据集(VIA标注的用这个)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjcN1KeoMopJ"
      },
      "source": [
        "def get_wear_dicts(img_dir):\r\n",
        "  json_file = os.path.join(img_dir, \"via_region_data.json\")\r\n",
        "  with open(json_file) as f:\r\n",
        "      imgs_anns = json.load(f)\r\n",
        "\r\n",
        "  dataset_dicts = []\r\n",
        "  for idx, v in enumerate(imgs_anns.values()):\r\n",
        "      record = {}\r\n",
        "      \r\n",
        "      filename = os.path.join(img_dir, v[\"filename\"])\r\n",
        "      height, width = cv2.imread(filename).shape[:2]\r\n",
        "      \r\n",
        "      record[\"file_name\"] = filename\r\n",
        "      record[\"image_id\"] = idx\r\n",
        "      record[\"height\"] = height\r\n",
        "      record[\"width\"] = width\r\n",
        "\r\n",
        "      list_annos = v[\"regions\"]\r\n",
        "\r\n",
        "      objs = []\r\n",
        "      # for _, anno in annos.items():\r\n",
        "      for dict_anno in list_annos:\r\n",
        "          # assert not anno[\"region_attributes\"]\r\n",
        "          anno = dict_anno[\"shape_attributes\"]\r\n",
        "          px = anno[\"all_points_x\"]\r\n",
        "          py = anno[\"all_points_y\"]\r\n",
        "          poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\r\n",
        "          poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "          # get type from region_attributes to set different category_id\r\n",
        "          attr1 = dict_anno[\"region_attributes\"]\r\n",
        "          type1 = attr1[\"type\"]\r\n",
        "\r\n",
        "          if type1 == \"abrasive wear\":\r\n",
        "              cat_id = 0\r\n",
        "          elif type1 == \"adhesive wear\":\r\n",
        "              cat_id = 1\r\n",
        "          elif type1 == \"fatigue wear\":\r\n",
        "              cat_id = 2\r\n",
        "          else:\r\n",
        "              cat_id = 0\r\n",
        "\r\n",
        "          obj = {\r\n",
        "              \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "              \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "              \"segmentation\": [poly],\r\n",
        "              \"category_id\": cat_id,\r\n",
        "          }\r\n",
        "          objs.append(obj)\r\n",
        "      record[\"annotations\"] = objs\r\n",
        "      dataset_dicts.append(record)\r\n",
        "  return dataset_dicts\r\n",
        "\r\n",
        "for d in [\"train\", \"val\"]:\r\n",
        "  DatasetCatalog.register(\"wear_\" + d, lambda d=d: get_wear_dicts(os.path.join(\"/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108\", d)))\r\n",
        "  MetadataCatalog.get(\"wear_\" + d).set(thing_classes=[\"abrasive wear\",\"adhesive wear\",\"fatigue wear\"])\r\n",
        "wear_metadata = MetadataCatalog.get(\"wear_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a83DdQfX2Ohl"
      },
      "source": [
        "### 2.1B Register with this one if you label with labelme\r\n",
        "### 2.1B 注册图像数据集(Labelme标注的用这个)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_RwaV02WzB"
      },
      "source": [
        "def get_wear_dicts(directory):\r\n",
        "  classes = ['abrasive wear', 'adhesive wear', 'fatigue wear']\r\n",
        "  dataset_dicts = []\r\n",
        "  for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\r\n",
        "      json_file = os.path.join(directory, filename)\r\n",
        "      with open(json_file) as f:\r\n",
        "          img_anns = json.load(f)\r\n",
        "\r\n",
        "      record = {}\r\n",
        "      \r\n",
        "      filename = os.path.join(directory, img_anns[\"imagePath\"])\r\n",
        "      \r\n",
        "      record[\"file_name\"] = filename\r\n",
        "      record[\"height\"] = 1088\r\n",
        "      record[\"width\"] = 1024\r\n",
        "    \r\n",
        "      annos = img_anns[\"shapes\"]\r\n",
        "      objs = []\r\n",
        "      for anno in annos:\r\n",
        "          px = [a[0] for a in anno['points']]\r\n",
        "          py = [a[1] for a in anno['points']]\r\n",
        "          poly = [(x, y) for x, y in zip(px, py)]\r\n",
        "          poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "          obj = {\r\n",
        "              \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "              \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "              \"segmentation\": [poly],\r\n",
        "              \"category_id\": classes.index(anno['label']),\r\n",
        "              \"iscrowd\": 0\r\n",
        "          }\r\n",
        "          objs.append(obj)\r\n",
        "      record[\"annotations\"] = objs\r\n",
        "      dataset_dicts.append(record)\r\n",
        "  return dataset_dicts\r\n",
        "\r\n",
        "for d in [\"train\", \"val\"]:\r\n",
        "  DatasetCatalog.register(\"wear_\" + d, lambda d=d: get_wear_dicts(\"/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/\" + d))\r\n",
        "  MetadataCatalog.get(\"wear_\" + d).set(thing_classes=['abrasive wear', 'adhesive wear', 'fatigue wear'])\r\n",
        "wear_metadata = MetadataCatalog.get(\"wear_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ4xBHryNNPi"
      },
      "source": [
        "### 2.2 Visualize the annotations of randomly selected samples in the training set\r\n",
        "### 2.2 随机预览训练集图像分割方式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wGY0W4TMur7"
      },
      "source": [
        "dataset_dicts = get_wear_dicts(\"/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train\")\r\n",
        "for d in random.sample(dataset_dicts, 1):\r\n",
        "    img = cv2.imread(d[\"file_name\"])\r\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wear_metadata, scale=0.5)\r\n",
        "    out = visualizer.draw_dataset_dict(d)\r\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_PqdEIbyiWO"
      },
      "source": [
        "### 2.3 Save labelled results of val dataset in folder\r\n",
        "### 2.3 保存验证集图片的标注结果输出到文件夹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artsWnHum_2x"
      },
      "source": [
        "def get_dir_true_info(directory):\r\n",
        "  os.mkdir(directory+'_true')\r\n",
        "  csv_file = open(directory+'_true/info.csv', 'w')\r\n",
        "  csv_file.close()\r\n",
        "  text_title = 'file_name,height,width,pixel,instance_num,abrasive_area,adhesive_area,fatigue_area'\r\n",
        "  with open(directory+'_true/info.csv','a+',encoding='utf-8') as f:\r\n",
        "    f.write(text_title+'\\n')\r\n",
        "    f.close()\r\n",
        "  dataset_dicts = get_wear_dicts(directory)\r\n",
        "  for d in dataset_dicts:\r\n",
        "    file = d[\"file_name\"]\r\n",
        "    file_name = file.split(\"/\")[-1]\r\n",
        "    true_name = directory+'_true/'+file_name\r\n",
        "    img = cv2.imread(d[\"file_name\"])\r\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wear_metadata, scale=1.0)\r\n",
        "    out = visualizer.draw_dataset_dict(d)\r\n",
        "    # cv2_imshow(out.get_image()[:, :, ::-1])\r\n",
        "    cv2.imwrite(true_name, out.get_image()[:, :, ::-1])\r\n",
        "  dir_info = []\r\n",
        "  for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\r\n",
        "    json_file = os.path.join(directory, filename)\r\n",
        "    with open(json_file) as f:\r\n",
        "      img_json = json.load(f)\r\n",
        "      f.close()\r\n",
        "    annos = img_json[\"shapes\"]\r\n",
        "    annos_num = len(annos)\r\n",
        "    img_name = img_json[\"imagePath\"]\r\n",
        "    img_height = img_json[\"imageHeight\"]\r\n",
        "    img_width = img_json[\"imageWidth\"]\r\n",
        "    img_pixel = img_height * img_width\r\n",
        "    abrasive_mask = np.zeros((img_height, img_width))\r\n",
        "    adhesive_mask = np.zeros((img_height, img_width))\r\n",
        "    fatigue_mask = np.zeros((img_height, img_width))   \r\n",
        "    for i, x in enumerate(annos):\r\n",
        "      polygon_mask = np.zeros((img_height, img_width))\r\n",
        "      points = x[\"points\"]\r\n",
        "      pts = np.array(points, dtype=np.int32)\r\n",
        "      cv2.fillConvexPoly(polygon_mask, pts, 1)\r\n",
        "      if x['label'] == 'abrasive wear':\r\n",
        "        abrasive_mask = np.array(abrasive_mask) + np.array(polygon_mask)\r\n",
        "        abrasive_mask[abrasive_mask > 0] = 1\r\n",
        "      elif x['label'] == 'adhesive wear':\r\n",
        "        adhesive_mask = np.array(adhesive_mask) + np.array(polygon_mask)\r\n",
        "        adhesive_mask[adhesive_mask > 0] = 1\r\n",
        "      elif x['label'] == 'fatigue wear':\r\n",
        "        fatigue_mask = np.array(fatigue_mask) + np.array(polygon_mask)\r\n",
        "        fatigue_mask[fatigue_mask > 0] = 1\r\n",
        "    abrasive_area = np.sum(abrasive_mask != 0)\r\n",
        "    adhesive_area = np.sum(adhesive_mask != 0)\r\n",
        "    fatigue_area = np.sum(fatigue_mask != 0)\r\n",
        "    image_info = {\"img_name\":img_name, \"abrasive_mask\":abrasive_mask,\"adhesive_mask\":adhesive_mask,\"fatigue_mask\":fatigue_mask}\r\n",
        "    dir_info.append(image_info)\r\n",
        "    true_text = str(img_name)+','+str(img_height)+','+str(img_width)+','+str(img_pixel)+','+str(annos_num)+','+str(abrasive_area)+','+str(adhesive_area)+','+str(fatigue_area)\r\n",
        "    with open((directory+'_true/info.csv'),'a+',encoding='utf-8') as f:\r\n",
        "      f.write(true_text+'\\n')\r\n",
        "      f.close()\r\n",
        "  return dir_info\r\n",
        "\r\n",
        "train_true_info = get_dir_true_info('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train')\r\n",
        "val_true_info = get_dir_true_info('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWz9S9EMk97"
      },
      "source": [
        "## 3. Mask R-CNN → Instance Segmentation\r\n",
        "## 3. Mask R-CNN实例分割训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvetkXuMNgg7"
      },
      "source": [
        "### 3.1 Train! (Or skip to use the pre-trained model)\r\n",
        "### 3.1 训练! (或跳过这一步直接使用已训练的模型）\r\n",
        "> \r\n",
        "\r\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPf4dH-yNbxh"
      },
      "source": [
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "cfg.DATASETS.TRAIN = (\"wear_train\",)\r\n",
        "cfg.DATASETS.TEST = ()\r\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\r\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") \r\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\r\n",
        "cfg.SOLVER.BASE_LR = 0.00025   # pick a good LR\r\n",
        "cfg.SOLVER.MAX_ITER = 10000   # you will need to train longer for a practical dataset\r\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # default: 512\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3   \r\n",
        "\r\n",
        "cfg.OUTPUT_DIR = '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/output'\r\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\r\n",
        "trainer = DefaultTrainer(cfg) \r\n",
        "trainer.resume_or_load(resume=False)\r\n",
        "trainer.train()\r\n",
        "print('train done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS_L19mSScuB"
      },
      "source": [
        "### 3.2 Check training curves\r\n",
        "### 3.2 查看训练曲线\r\n",
        "> Check training curves of pre-trained model by default\r\n",
        "\r\n",
        "> 默认查看已训练好了的模型曲线"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HO0Rcy7STRS"
      },
      "source": [
        "# Look at training curves in tensorboard:\r\n",
        "%load_ext tensorboard\r\n",
        "# %tensorboard --logdir '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/output'\r\n",
        "%tensorboard --logdir '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/WC-Cu_20210108_output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1WQjROTN-e7"
      },
      "source": [
        "### 3.3 Inference! (The pre-trained model is applied by default)\r\n",
        "### 3.3 预测! (默认使用已训练过的模型预测)\r\n",
        "> \r\n",
        "\r\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ML7aT86_oZ-"
      },
      "source": [
        "def get_dir_pred_info(directory):\r\n",
        "  os.mkdir(directory+'_pred')\r\n",
        "  csv_file = open(directory+'_pred/info.csv', 'w')\r\n",
        "  text_title = 'file_name,height,width,pixel,instance_num,abrasive_area,adhesive_area,fatigue_area'\r\n",
        "  with open(directory+'_pred/info.csv','a+',encoding='utf-8') as f:\r\n",
        "    f.write(text_title+'\\n')\r\n",
        "    f.close()\r\n",
        "  dir_info = []\r\n",
        "  files = os.listdir(directory)\r\n",
        "  files.sort()\r\n",
        "  for file_name in files:\r\n",
        "    # filter jpg and tiff files\r\n",
        "    if file_name[-4:] == '.jpg' or file_name[-5:] == '.tiff':\r\n",
        "      image_path = os.path.join(directory, file_name)\r\n",
        "      im = cv2.imread(image_path)\r\n",
        "      outputs = predictor(im)          \r\n",
        "      predictions = outputs[\"instances\"].to(\"cpu\")\r\n",
        "      predictions_classes = np.asarray(predictions.pred_classes)\r\n",
        "      predictions_scores = np.asarray(predictions.scores)\r\n",
        "      predictions_masks = np.asarray(predictions.pred_masks)\r\n",
        "      predictions_areas = []\r\n",
        "      predictions_instance_num = predictions_masks.shape[0]\r\n",
        "      predictions_height = predictions_masks.shape[1]\r\n",
        "      predictions_width = predictions_masks.shape[2]\r\n",
        "      predictions_pixel = predictions_height * predictions_width          \r\n",
        "      blank_mask = np.zeros((predictions_height, predictions_width), dtype=bool)\r\n",
        "      predictions_abrasive_mask = blank_mask\r\n",
        "      predictions_adhesive_mask = blank_mask\r\n",
        "      predictions_fatigue_mask = blank_mask\r\n",
        "      for i in range(0, predictions_instance_num):\r\n",
        "        instance_num = i\r\n",
        "        instance_class = predictions_classes[i]\r\n",
        "        instance_score = predictions_scores[i]\r\n",
        "        instance_mask = predictions_masks[i]\r\n",
        "        instance_area = np.sum(instance_mask != 0)\r\n",
        "        predictions_areas.append(instance_area)\r\n",
        "        if predictions_classes[i] == 0:\r\n",
        "          predictions_abrasive_mask = np.array(predictions_abrasive_mask) + np.array(instance_mask)\r\n",
        "          predictions_abrasive_mask[predictions_abrasive_mask > 0] = 1\r\n",
        "        elif predictions_classes[i] == 1:\r\n",
        "          predictions_adhesive_mask = np.array(predictions_adhesive_mask) + np.array(instance_mask)\r\n",
        "          predictions_adhesive_mask[predictions_adhesive_mask > 0] = 1\r\n",
        "        elif predictions_classes[i] == 2:\r\n",
        "          predictions_fatigue_mask = np.array(predictions_fatigue_mask) + np.array(instance_mask)\r\n",
        "          predictions_fatigue_mask[predictions_fatigue_mask > 0] = 1       \r\n",
        "      predictions_abrasive_area = np.sum(predictions_abrasive_mask != 0)\r\n",
        "      predictions_adhesive_area = np.sum(predictions_adhesive_mask != 0)\r\n",
        "      predictions_fatigue_area = np.sum(predictions_fatigue_mask != 0)\r\n",
        "      image_info = {\"img_name\":file_name, \"abrasive_mask\":predictions_abrasive_mask,\"adhesive_mask\":predictions_adhesive_mask,\"fatigue_mask\":predictions_fatigue_mask}\r\n",
        "      dir_info.append(image_info)\r\n",
        "      predictions_text = str(file_name)+','+str(predictions_height)+','+str(predictions_width)+','+str(predictions_pixel)+','+str(predictions_instance_num)+','+str(predictions_abrasive_area)+','+str(predictions_adhesive_area)+','+str(predictions_fatigue_area)\r\n",
        "      with open(directory+'_pred/info.csv','a+',encoding='utf-8') as f:\r\n",
        "        f.write(predictions_text+'\\n')\r\n",
        "        f.close()\r\n",
        "      v = Visualizer(im[:,:,::-1], metadata=wear_metadata, scale=1.0, instance_mode=ColorMode.IMAGE_BW)\r\n",
        "      out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\r\n",
        "      image_obj = out.get_image()[:, :, ::-1]\r\n",
        "      # cv2_imshow(image_obj)\r\n",
        "      cv2.imwrite(os.path.join((directory+'_pred'), file_name), image_obj)\r\n",
        "  return dir_info\r\n",
        "\r\n",
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  \r\n",
        "cfg.MODEL.WEIGHTS = os.path.join('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/WC-Cu_20210108_output', \"model_final.pth\")\r\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\r\n",
        "predictor = DefaultPredictor(cfg)\r\n",
        "\r\n",
        "train_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train')\r\n",
        "val_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/val')\r\n",
        "test_pred_info = get_dir_pred_info('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9GtjShY10lQ"
      },
      "source": [
        "### 3.4 Evaluate inferencing results of all wear modes\r\n",
        "### 3.4 评价各磨损类型预测效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYLet_krQBH"
      },
      "source": [
        "!touch '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train_count.csv'\n",
        "!touch '/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/val_count.csv'\n",
        "text_title = 'img_name,abrasive_mask_iou,adhesive_mask_iou,fatigue_mask_iou,abrasive_area_loss,adhesive_area_loss,fatigue_area_loss'\n",
        "with open('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train_count.csv','a+',encoding='utf-8') as f:\n",
        "  f.write(text_title+'\\n')\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/val_count.csv','a+',encoding='utf-8') as f:\n",
        "  f.write(text_title+'\\n')\n",
        "  f.close()\n",
        "\n",
        "def mask_iou(mask1, mask2):\n",
        "  area1 = mask1.sum()\n",
        "  area2 = mask2.sum()\n",
        "  if area1 + area2 == 0:\n",
        "    mask_iou = 1\n",
        "  else:\n",
        "    inter = ((mask1+mask2)==2).sum()\n",
        "    mask_iou = inter / (area1+area2-inter)\n",
        "  return mask_iou\n",
        "\n",
        "def area_loss(mask1, mask2):\n",
        "  area1 = mask1.sum()\n",
        "  area2 = mask2.sum()\n",
        "  if area1 + area2 == 0:\n",
        "    area_loss = 0\n",
        "  else:\n",
        "    area_loss = abs(area1 - area2)/(area1 + area2)\n",
        "  return area_loss\n",
        "\n",
        "for i, x in enumerate(train_true_info):\n",
        "  img_name = x[\"img_name\"]\n",
        "  true_abrasive_mask = x[\"abrasive_mask\"]\n",
        "  true_adhesive_mask = x[\"adhesive_mask\"]\n",
        "  true_fatigue_mask = x[\"fatigue_mask\"]\n",
        "  pred_abrasive_mask = train_pred_info[i][\"abrasive_mask\"]\n",
        "  pred_adhesive_mask = train_pred_info[i][\"adhesive_mask\"]\n",
        "  pred_fatigue_mask = train_pred_info[i][\"fatigue_mask\"]\n",
        "  train_abrasive_iou = mask_iou(true_abrasive_mask, pred_abrasive_mask)\n",
        "  train_adhesive_iou = mask_iou(true_adhesive_mask, pred_adhesive_mask)\n",
        "  train_fatigue_iou = mask_iou(true_fatigue_mask, pred_fatigue_mask)\n",
        "  train_abrasive_loss = area_loss(true_abrasive_mask, pred_abrasive_mask)\n",
        "  train_adhesive_loss = area_loss(true_adhesive_mask, pred_adhesive_mask)\n",
        "  train_fatigue_loss = area_loss(true_fatigue_mask, pred_fatigue_mask)\n",
        "  count_text = str(img_name)+','+str(train_abrasive_iou)+','+str(train_adhesive_iou)+','+str(train_fatigue_iou)+','+str(train_abrasive_loss)+','+str(train_adhesive_loss)+','+str(train_fatigue_loss)\n",
        "  with open('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/train_count.csv','a+',encoding='utf-8') as f:\n",
        "    f.write(count_text+'\\n')\n",
        "    f.close()\n",
        "\n",
        "for i, x in enumerate(val_true_info):\n",
        "  img_name = x[\"img_name\"]\n",
        "  true_abrasive_mask = x[\"abrasive_mask\"]\n",
        "  true_adhesive_mask = x[\"adhesive_mask\"]\n",
        "  true_fatigue_mask = x[\"fatigue_mask\"]\n",
        "  pred_abrasive_mask = val_pred_info[i][\"abrasive_mask\"]\n",
        "  pred_adhesive_mask = val_pred_info[i][\"adhesive_mask\"]\n",
        "  pred_fatigue_mask = val_pred_info[i][\"fatigue_mask\"]\n",
        "  val_abrasive_iou = mask_iou(true_abrasive_mask, pred_abrasive_mask)\n",
        "  val_adhesive_iou = mask_iou(true_adhesive_mask, pred_adhesive_mask)\n",
        "  val_fatigue_iou = mask_iou(true_fatigue_mask, pred_fatigue_mask)\n",
        "  val_abrasive_loss = area_loss(true_abrasive_mask, pred_abrasive_mask)\n",
        "  val_adhesive_loss = area_loss(true_adhesive_mask, pred_adhesive_mask)\n",
        "  val_fatigue_loss = area_loss(true_fatigue_mask, pred_fatigue_mask)\n",
        "  count_text = str(img_name)+','+str(val_abrasive_iou)+','+str(val_adhesive_iou)+','+str(val_fatigue_iou)+','+str(val_abrasive_loss)+','+str(val_adhesive_loss)+','+str(val_fatigue_loss)\n",
        "  with open('/content/drive/MyDrive/IDB_Wear/WC-Cu_20210108/val_count.csv','a+',encoding='utf-8') as f:\n",
        "    f.write(count_text+'\\n')\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK4jJaOXOKCz"
      },
      "source": [
        "## 4. Other models\r\n",
        "## 4. 其他模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHq30lwPCX9"
      },
      "source": [
        "### 4.1 关键点检测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hAIpF9FPG4F"
      },
      "source": [
        "# Inference with a keypoint detection model\r\n",
        "cfg = get_cfg()   # get a fresh new config\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\r\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\r\n",
        "predictor = DefaultPredictor(cfg)\r\n",
        "outputs = predictor(im)\r\n",
        "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\r\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\r\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ265YH8PIgj"
      },
      "source": [
        "### 4.2 全景分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hloc6p9DOSAM"
      },
      "source": [
        "# Inference with a panoptic segmentation model\r\n",
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\r\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\r\n",
        "predictor = DefaultPredictor(cfg)\r\n",
        "panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\r\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\r\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\r\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-iul0ozPO-r"
      },
      "source": [
        "### 4.3 视频全景分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI1YOPJaPRhF"
      },
      "source": [
        "# This is the video we're going to process\r\n",
        "from IPython.display import YouTubeVideo, display\r\n",
        "video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)\r\n",
        "display(video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtnarpbEPTIM"
      },
      "source": [
        "# Install dependencies, download the video, and crop 5 seconds for processing\r\n",
        "!pip install youtube-dl\r\n",
        "!pip uninstall -y opencv-python-headless opencv-contrib-python\r\n",
        "!apt install python3-opencv  # the one pre-installed have some issues\r\n",
        "!youtube-dl https://www.youtube.com/watch?v=ll8TgCZ0plk -f 22 -o video.mp4\r\n",
        "!ffmpeg -i video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSadXlnGPW5y"
      },
      "source": [
        "# Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the \"demo.py\" tool we provided in the repo.\r\n",
        "!git clone https://github.com/facebookresearch/detectron2\r\n",
        "!python detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv \\\r\n",
        "  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyBzMhjnPXSb"
      },
      "source": [
        "# Download the results\r\n",
        "from google.colab import files\r\n",
        "files.download('video-output.mkv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}